{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/workspaces/mnist/data')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path.cwd().parent / \"data\"\n",
    "\n",
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ylecun/mnist\")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': Image(mode=None, decode=True, id=None),\n",
       " 'label': ClassLabel(names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ds[\"train\"][0]\n",
    "\n",
    "example[\"image\"].resize((128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ds[\"train\"].features[\"label\"].names\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAcABwDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDwAAswVQSScADvXZ6f8JvHWprug8OXUYxn/SCsP/oZFcWDg5FeveFvHOueLfDOp+E7rxHLa6xcNHLp93NP5avtwDCXAyuQMjnk5HfkA5y++FOsaS12uqaroVi1rCJXWa9+Zs5OxVAJLYAPTHI5rhK3fGFn4gsvEtzF4m89tUG1ZJJjuMgUBVYN/EMKOe9YVAF7StG1LXb1bPS7Ge8uGIGyFC2MnGT6D3PFepaX8IdP8O6eus/EXV49Mts/JZQuGlkPpkZ/Jc/UV5bputaroskkmlaneWLyDa7Ws7RFh6EqRmoby+vNQm869up7mXpvmkLt69T9TQB2/wATfHWm+LjpNho9lNBp2kwmCGW5YNLKuFAz1IwF7k5zk1wFFFAH/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAB/UlEQVR4Ae1UPajBURT3lZJCb/FVFoNBlEFKBhmMIiZlkY3JYmNSBpNkkMFkoQzKQBRKGQwGJBkIq+9EHu+8/j2ur7/73vhyh9u55/x+v865955DobzX/7kBOp3+8bP8fn8oFMpmsyKRKJVKnU6n7XYbCATQahnoAbUlEgmTydRqtTqdjsfjWa1WNDoejyORiMViWa1WrVarUqmgUSp6ONsqlapUKnG53LMHNY7Ho9Pp3Gw24JxOp7PZrNfroYDHNtTa7/c/r1e9Xs/n81DsYrF4THvpNZvNiUTC7XYTys1mk81mA0sul8fj8Zf0pwAOh0OlUkECdO12+1PcXYB257k4lsslPC5RrMvlotHIwBcajgVVl8tlSNZoNOLgcTFSqRTyHQ6HyWTS4/HAneAyyXHwJefzOfFoPp9PKBSS43GjCoWiUCgQurFYTCwW4zLJcdBaDofjcDiAdLFYJAf/Lrrb7UAUdr1e/5D5tPfv0Uql0mazqdVqBuOb1el0qtXqPQzXI5PJotHoZDIhLhT2/X4PLYvLv8EJBAKv1zsYDM5yYDQaDZPJdIPEOvL5fIPB0G63UTmYKfC3/tJaMKLS6fTNlKrVajBiWCwWVkYoSKPRZDKZ0WiEZrder4PBIDGiUDCJffX6UBosAt3tdnO5HKiHw2HoJRKJd+jmBr4Am/Abbz4sS3cAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print image channels\n",
    "example[\"image\"].convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0,\n",
       " '1': 1,\n",
       " '2': 2,\n",
       " '3': 3,\n",
       " '4': 4,\n",
       " '5': 5,\n",
       " '6': 6,\n",
       " '7': 7,\n",
       " '8': 8,\n",
       " '9': 9}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessorFast {\n",
       "  \"do_convert_rgb\": null,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessorFast\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "# https://huggingface.co/google/vit-base-patch16-224#preprocessing\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\", use_fast=True)\n",
    "\n",
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "base_model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224',\n",
    "    num_labels=10,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test to ensure dataset is consumable by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = ds[\"train\"][0][\"image\"]\n",
    "\n",
    "img.resize((128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = processor(img, return_tensors=\"pt\")\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[ 0.1310,  0.6511, -0.1524, -0.2226, -0.2744,  0.5164, -0.2700,  0.0285,\n",
       "          0.5215,  1.1842]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = base_model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_idx = outputs.logits.argmax(-1).item()\n",
    "\n",
    "id2label[cls_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    # convert images to pixels tensor\n",
    "    inputs = processor(batch[\"image\"], return_tensors=\"pt\")\n",
    "\n",
    "    return {\n",
    "        \"image\": batch[\"image\"],\n",
    "        \"pixel_values\": inputs[\"pixel_values\"],\n",
    "        \"label\": batch[\"label\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 224, 224]), [5, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_ds = ds.with_transform(preprocess)\n",
    "\n",
    "x = prepared_ds['train'][0:2]\n",
    "\n",
    "x[\"pixel_values\"].shape, x[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = torch.argmax(torch.tensor(p.predictions), dim=1)\n",
    "    return metric.compute(predictions=preds, references=p.label_ids)\n",
    "\n",
    "def collate(batches):\n",
    "    pixel_values = torch.stack([batch[\"pixel_values\"] for batch in batches])\n",
    "    labels = torch.tensor([batch[\"label\"] for batch in batches])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for param in base_model.vit.parameters():\n",
    "    param.requires_grad = False  # freeze vit backbone\n",
    "\n",
    "for param in base_model.classifier.parameters():\n",
    "    param.requires_grad = True  # train only classifier\n",
    "    \n",
    "target_params = [name for name, param in base_model.named_parameters() if param.requires_grad]\n",
    "for name in target_params:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7690"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=DATA_DIR / \"models\" / \"google-vit-patch16-224\" / \"ft\",\n",
    "    run_name=\"google-vit-patch16-224-ft\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=1e-3,\n",
    "    torch_compile=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=DATA_DIR / \"logs\" / \"google-vit-patch16-224\" / \"ft\",\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False, # preserves \"image\" column for preprocessing\n",
    "    data_seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    processing_class=processor,\n",
    "    args=training_args,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"test\"],\n",
    "    data_collator=collate,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.get_num_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrbrobot\u001b[0m (\u001b[33mcloudbend\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspaces/mnist/nbs/wandb/run-20250215_223031-ehs171vh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cloudbend/huggingface/runs/ehs171vh' target=\"_blank\">google-vit-patch16-224-ft</a></strong> to <a href='https://wandb.ai/cloudbend/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cloudbend/huggingface' target=\"_blank\">https://wandb.ai/cloudbend/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cloudbend/huggingface/runs/ehs171vh' target=\"_blank\">https://wandb.ai/cloudbend/huggingface/runs/ehs171vh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 05:51, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.175300</td>\n",
       "      <td>0.151133</td>\n",
       "      <td>0.954200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.121400</td>\n",
       "      <td>0.126409</td>\n",
       "      <td>0.962300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "ft_model = trainer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier/Encoder Coadaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit.encoder.layer.11.attention.attention.query.weight\n",
      "vit.encoder.layer.11.attention.attention.query.bias\n",
      "vit.encoder.layer.11.attention.attention.key.weight\n",
      "vit.encoder.layer.11.attention.attention.key.bias\n",
      "vit.encoder.layer.11.attention.attention.value.weight\n",
      "vit.encoder.layer.11.attention.attention.value.bias\n",
      "vit.encoder.layer.11.attention.output.dense.weight\n",
      "vit.encoder.layer.11.attention.output.dense.bias\n",
      "vit.encoder.layer.11.intermediate.dense.weight\n",
      "vit.encoder.layer.11.intermediate.dense.bias\n",
      "vit.encoder.layer.11.output.dense.weight\n",
      "vit.encoder.layer.11.output.dense.bias\n",
      "vit.encoder.layer.11.layernorm_before.weight\n",
      "vit.encoder.layer.11.layernorm_before.bias\n",
      "vit.encoder.layer.11.layernorm_after.weight\n",
      "vit.encoder.layer.11.layernorm_after.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for param in ft_model.vit.parameters():\n",
    "    param.requires_grad = False  # freeze vit backbone\n",
    "\n",
    "for param in ft_model.vit.encoder.layer[-1].parameters():\n",
    "    param.requires_grad = True  # unfreeze last encoder layer\n",
    "\n",
    "for param in ft_model.classifier.parameters():\n",
    "    param.requires_grad = True  # unfreeze classifier\n",
    "    \n",
    "target_params = [name for name, param in ft_model.named_parameters() if param.requires_grad]\n",
    "for name in target_params:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7095562"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=DATA_DIR / \"models\" / \"google-vit-patch16-224\" / \"ft\",\n",
    "    run_name=\"google-vit-patch16-224-ft\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=1e-3,\n",
    "    torch_compile=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=DATA_DIR / \"logs\" / \"google-vit-patch16-224\" / \"ft\",\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False, # preserves \"image\" column for preprocessing\n",
    "    data_seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=ft_model,\n",
    "    processing_class=processor,\n",
    "    args=training_args,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"test\"],\n",
    "    data_collator=collate,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.get_num_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 05:33, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.119200</td>\n",
       "      <td>0.116543</td>\n",
       "      <td>0.963500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.092600</td>\n",
       "      <td>0.102577</td>\n",
       "      <td>0.968300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "ft_model = trainer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8, # rank of update matrices\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"query\", \"key\", \"value\"], # layers to apply lora to\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(ft_model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442368"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=DATA_DIR / \"models\" / \"google-vit-patch16-224\" / \"peft\",\n",
    "    run_name=\"google-vit-patch16-224-peft\",\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=1e-3,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    # load_best_model_at_end=True, # currently casues KeyError: 'eval_loss' after 1st epoch\n",
    "    logging_dir=DATA_DIR / \"logs\" / \"google-vit-patch16-224\" / \"peft\",\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False, # preserves \"image\" column for preprocessing\n",
    "    data_seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    processing_class=processor,\n",
    "    args=training_args,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"test\"],\n",
    "    data_collator=collate,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.get_num_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 21:50, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9958"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import evaluator\n",
    "from transformers import pipeline\n",
    "\n",
    "eval = evaluator(\"image-classification\")\n",
    "\n",
    "pipe = pipeline(\"image-classification\", model=model, feature_extractor=processor, use_fast=True)\n",
    "\n",
    "result = eval.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=ds[\"test\"],\n",
    "    metric=\"accuracy\",\n",
    "    input_column=\"image\",\n",
    "    label_column=\"label\",\n",
    "    label_mapping=label2id,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "result[\"accuracy\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
