{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/workspaces/mnist/data')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path.cwd().parent / \"data\"\n",
    "\n",
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ylecun/mnist\")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': Image(mode=None, decode=True, id=None),\n",
       " 'label': ClassLabel(names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ds[\"train\"][0]\n",
    "\n",
    "example[\"image\"].resize((128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ds[\"train\"].features[\"label\"].names\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAcABwDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDwAAswVQSScADvXZ6f8JvHWprug8OXUYxn/SCsP/oZFcWDg5FeveFvHOueLfDOp+E7rxHLa6xcNHLp93NP5avtwDCXAyuQMjnk5HfkA5y++FOsaS12uqaroVi1rCJXWa9+Zs5OxVAJLYAPTHI5rhK3fGFn4gsvEtzF4m89tUG1ZJJjuMgUBVYN/EMKOe9YVAF7StG1LXb1bPS7Ge8uGIGyFC2MnGT6D3PFepaX8IdP8O6eus/EXV49Mts/JZQuGlkPpkZ/Jc/UV5bputaroskkmlaneWLyDa7Ws7RFh6EqRmoby+vNQm869up7mXpvmkLt69T9TQB2/wATfHWm+LjpNho9lNBp2kwmCGW5YNLKuFAz1IwF7k5zk1wFFFAH/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAB/UlEQVR4Ae1UPajBURT3lZJCb/FVFoNBlEFKBhmMIiZlkY3JYmNSBpNkkMFkoQzKQBRKGQwGJBkIq+9EHu+8/j2ur7/73vhyh9u55/x+v865955DobzX/7kBOp3+8bP8fn8oFMpmsyKRKJVKnU6n7XYbCATQahnoAbUlEgmTydRqtTqdjsfjWa1WNDoejyORiMViWa1WrVarUqmgUSp6ONsqlapUKnG53LMHNY7Ho9Pp3Gw24JxOp7PZrNfroYDHNtTa7/c/r1e9Xs/n81DsYrF4THvpNZvNiUTC7XYTys1mk81mA0sul8fj8Zf0pwAOh0OlUkECdO12+1PcXYB257k4lsslPC5RrMvlotHIwBcajgVVl8tlSNZoNOLgcTFSqRTyHQ6HyWTS4/HAneAyyXHwJefzOfFoPp9PKBSS43GjCoWiUCgQurFYTCwW4zLJcdBaDofjcDiAdLFYJAf/Lrrb7UAUdr1e/5D5tPfv0Uql0mazqdVqBuOb1el0qtXqPQzXI5PJotHoZDIhLhT2/X4PLYvLv8EJBAKv1zsYDM5yYDQaDZPJdIPEOvL5fIPB0G63UTmYKfC3/tJaMKLS6fTNlKrVajBiWCwWVkYoSKPRZDKZ0WiEZrder4PBIDGiUDCJffX6UBosAt3tdnO5HKiHw2HoJRKJd+jmBr4Am/Abbz4sS3cAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print image channels\n",
    "example[\"image\"].convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0,\n",
       " '1': 1,\n",
       " '2': 2,\n",
       " '3': 3,\n",
       " '4': 4,\n",
       " '5': 5,\n",
       " '6': 6,\n",
       " '7': 7,\n",
       " '8': 8,\n",
       " '9': 9}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessorFast {\n",
       "  \"do_convert_rgb\": null,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessorFast\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "# https://huggingface.co/google/vit-base-patch16-224#preprocessing\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\", use_fast=True)\n",
    "\n",
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "base_model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224',\n",
    "    num_labels=10,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test to ensure dataset is consumable by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = ds[\"train\"][0][\"image\"]\n",
    "\n",
    "img.resize((128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = processor(img, return_tensors=\"pt\")\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[ 0.4121,  0.0036,  0.3051, -1.1191, -0.4584,  0.2864,  0.5081,  0.2331,\n",
       "          0.4605,  0.2971]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = base_model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_idx = outputs.logits.argmax(-1).item()\n",
    "\n",
    "id2label[cls_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    # convert images to pixels tensor\n",
    "    inputs = processor(batch[\"image\"], return_tensors=\"pt\")\n",
    "\n",
    "    return {\n",
    "        \"image\": batch[\"image\"],\n",
    "        \"pixel_values\": inputs[\"pixel_values\"],\n",
    "        \"label\": batch[\"label\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 224, 224]), [5, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_ds = ds.with_transform(preprocess)\n",
    "\n",
    "x = prepared_ds['train'][0:2]\n",
    "\n",
    "x[\"pixel_values\"].shape, x[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = torch.argmax(torch.tensor(p.predictions), dim=1)\n",
    "    return metric.compute(predictions=preds, references=p.label_ids)\n",
    "\n",
    "def collate(batches):\n",
    "    pixel_values = torch.stack([batch[\"pixel_values\"] for batch in batches])\n",
    "    labels = torch.tensor([batch[\"label\"] for batch in batches])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier\n",
      "Linear(in_features=768, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for n, c in base_model.named_children():\n",
    "    if n == \"classifier\":\n",
    "        print(n)\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target parameters:\n",
      "- vit.encoder.layer.11.attention.attention.query.weight\n",
      "- vit.encoder.layer.11.attention.attention.query.bias\n",
      "- vit.encoder.layer.11.attention.attention.key.weight\n",
      "- vit.encoder.layer.11.attention.attention.key.bias\n",
      "- vit.encoder.layer.11.attention.attention.value.weight\n",
      "- vit.encoder.layer.11.attention.attention.value.bias\n",
      "- vit.encoder.layer.11.attention.output.dense.weight\n",
      "- vit.encoder.layer.11.attention.output.dense.bias\n",
      "- vit.encoder.layer.11.intermediate.dense.weight\n",
      "- vit.encoder.layer.11.intermediate.dense.bias\n",
      "- vit.encoder.layer.11.output.dense.weight\n",
      "- vit.encoder.layer.11.output.dense.bias\n",
      "- vit.encoder.layer.11.layernorm_before.weight\n",
      "- vit.encoder.layer.11.layernorm_before.bias\n",
      "- vit.encoder.layer.11.layernorm_after.weight\n",
      "- vit.encoder.layer.11.layernorm_after.bias\n",
      "- classifier.weight\n",
      "- classifier.bias\n"
     ]
    }
   ],
   "source": [
    "# freeze all parameters except for final encoder layer and classifier\n",
    "for name, param in base_model.named_parameters():\n",
    "    if not name.startswith(\"classifier.\") and not name.startswith(\"vit.encoder.layer.11.\"):\n",
    "        param.requires_grad = False\n",
    "\n",
    "target_params = [name for name, param in base_model.named_parameters() if param.requires_grad]\n",
    "print(\"Target parameters:\")\n",
    "for name in target_params:\n",
    "    print(f\"- {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total target parameters: 7095562'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_target_parameters = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
    "\n",
    "f\"Total target parameters: {total_target_parameters}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrbrobot\u001b[0m (\u001b[33mcloudbend\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspaces/mnist/nbs/wandb/run-20250213_215324-akfnj1mt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cloudbend/huggingface/runs/akfnj1mt' target=\"_blank\">google-vit-patch16-224-ft</a></strong> to <a href='https://wandb.ai/cloudbend/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cloudbend/huggingface' target=\"_blank\">https://wandb.ai/cloudbend/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cloudbend/huggingface/runs/akfnj1mt' target=\"_blank\">https://wandb.ai/cloudbend/huggingface/runs/akfnj1mt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 06:29, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.069391</td>\n",
       "      <td>0.978200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.046337</td>\n",
       "      <td>0.986700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3750, training_loss=0.08011900926828384, metrics={'train_runtime': 404.5232, 'train_samples_per_second': 296.646, 'train_steps_per_second': 9.27, 'total_flos': 9.29970550849536e+18, 'train_loss': 0.08011900926828384, 'epoch': 2.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# before - 13:11 training time, 98.82% accuracy\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=DATA_DIR / \"models\" / \"google-vit-patch16-224\" / \"ft\",\n",
    "    run_name=\"google-vit-patch16-224-ft\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=1e-3,\n",
    "    torch_compile=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=DATA_DIR / \"logs\" / \"google-vit-patch16-224\" / \"ft\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False, # preserves \"image\" column for preprocessing\n",
    "    data_seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    processing_class=processor,\n",
    "    args=training_args,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"test\"],\n",
    "    data_collator=collate,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 442368\n",
      "Total params: 86248714\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8, # rank of update matrices\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"], # layers to apply lora to\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(base_model, peft_config)\n",
    "\n",
    "print(f\"Trainable params: {sum(p.numel() for p in peft_model.parameters() if p.requires_grad)}\")\n",
    "print(f\"Total params: {sum(p.numel() for p in peft_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 17:51, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3750, training_loss=0.031837239613508185, metrics={'train_runtime': 1094.6383, 'train_samples_per_second': 109.625, 'train_steps_per_second': 3.426, 'total_flos': 9.34764942311424e+18, 'train_loss': 0.031837239613508185, 'epoch': 2.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=DATA_DIR / \"models\" / \"google-vit-patch16-224\" / \"peft\",\n",
    "    run_name=\"google-vit-patch16-224-peft\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=1e-3,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    # load_best_model_at_end=True, # currently casues KeyError: 'eval_loss' after 1st epoch\n",
    "    logging_dir=DATA_DIR / \"logs\" / \"google-vit-patch16-224\" / \"peft\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False, # preserves \"image\" column for preprocessing\n",
    "    data_seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    processing_class=processor,\n",
    "    args=training_args,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"test\"],\n",
    "    data_collator=collate,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get accuracy\n",
    "model = peft_model.merge_and_unload()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please specify a valid `data` object - either a `str` with a name or a `Dataset` object.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, feature_extractor\u001b[38;5;241m=\u001b[39mprocessor, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28meval\u001b[39m \u001b[38;5;241m=\u001b[39m evaluator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_or_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel2id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.8/lib/python3.12/site-packages/evaluate/evaluator/image_classification.py:101\u001b[0m, in \u001b[0;36mImageClassificationEvaluator.compute\u001b[0;34m(self, model_or_pipeline, data, subset, split, metric, tokenizer, feature_extractor, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, label_mapping)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;129m@add_start_docstrings\u001b[39m(EVALUTOR_COMPUTE_START_DOCSTRING)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(EVALUATOR_COMPUTE_RETURN_DOCSTRING, TASK_DOCUMENTATION)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m     label_mapping: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Number]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     89\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], Any]:\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m    input_column (`str`, defaults to `\"image\"`):\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m        The name of the column containing the images as PIL ImageFile in the dataset specified by `data`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m        defined in the `label_column` of the `data` dataset.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_or_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_or_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfidence_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfidence_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_resamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_resamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/python/3.12.8/lib/python3.12/site-packages/evaluate/evaluator/base.py:244\u001b[0m, in \u001b[0;36mEvaluator.compute\u001b[0;34m(self, model_or_pipeline, data, subset, split, metric, tokenizer, feature_extractor, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, label_mapping)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_for_mismatch_in_device_setup(device, model_or_pipeline)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Prepare inputs\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m metric_inputs, pipe_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_data(data\u001b[38;5;241m=\u001b[39mdata, input_column\u001b[38;5;241m=\u001b[39minput_column, label_column\u001b[38;5;241m=\u001b[39mlabel_column)\n\u001b[1;32m    246\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_pipeline(\n\u001b[1;32m    247\u001b[0m     model_or_pipeline\u001b[38;5;241m=\u001b[39mmodel_or_pipeline,\n\u001b[1;32m    248\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m    249\u001b[0m     feature_extractor\u001b[38;5;241m=\u001b[39mfeature_extractor,\n\u001b[1;32m    250\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    251\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/python/3.12.8/lib/python3.12/site-packages/evaluate/evaluator/base.py:386\u001b[0m, in \u001b[0;36mEvaluator.load_data\u001b[0;34m(self, data, subset, split)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease specify a valid `data` object - either a `str` with a name or a `Dataset` object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    388\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Please specify a valid `data` object - either a `str` with a name or a `Dataset` object."
     ]
    }
   ],
   "source": [
    "from evaluate import evaluator\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-classification\", model=model, feature_extractor=processor, use_fast=True)\n",
    "eval = evaluator(\"image-classification\")\n",
    "\n",
    "eval.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=prepared_ds[\"test\"],\n",
    "    metric=\"accuracy\",\n",
    "    input_column=\"image\",\n",
    "    label_column=\"label\",\n",
    "    label_mapping=label2id,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
